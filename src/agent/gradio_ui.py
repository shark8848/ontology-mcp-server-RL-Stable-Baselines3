#!/usr/bin/env python3
from __future__ import annotations
# Copyright (c) 2025 shark8848
# MIT License
#
# Ontology MCP Server - ç”µå•† AI åŠ©æ‰‹ç³»ç»Ÿ
# æœ¬ä½“æ¨ç† + ç”µå•†ä¸šåŠ¡é€»è¾‘ + å¯¹è¯è®°å¿† + å¯è§†åŒ– UI
#
# Author: shark8848
# Repository: https://github.com/shark8848/ontology-mcp-server
"""Gradio UI for the LangChain-based agent (uses OpenAI-compatible LLM + MCP HTTP adapter).
Run this script from repository root; it will talk to the MCP server at MCP_BASE_URL (default http://localhost:8000).

æ”¯æŒ ChromaDB æŒä¹…åŒ–å¯¹è¯è®°å¿†åŠŸèƒ½,å¯ä»¥ä¿æŒä¸Šä¸‹æ–‡è¿è´¯æ€§å¹¶æ”¯æŒè¯­ä¹‰æ£€ç´¢ã€‚
å¢å¼ºçš„è¿è¡Œæ—¥å¿—æ˜¾ç¤º,åŒ…æ‹¬æ¯ä¸ªç¯èŠ‚çš„è¾“å…¥è¾“å‡ºå’Œæ¨¡å‹äº¤äº’ç»†èŠ‚ã€‚
"""

import os
import uuid
import json
from pathlib import Path

import gradio as gr
import yaml

from agent.react_agent import LangChainAgent
from agent.logger import get_logger
from agent.memory_config import get_memory_config

LOGGER = get_logger(__name__)

# åŠ è½½è®°å¿†é…ç½®
MEMORY_CONFIG = get_memory_config()


def _load_agent_config() -> dict:
    config_path = Path(__file__).resolve().parent / "config.yaml"
    if not config_path.exists():
        return {}
    try:
        with config_path.open("r", encoding="utf-8") as fh:
            data = yaml.safe_load(fh) or {}
            if isinstance(data, dict):
                return data
    except Exception as exc:  # pragma: no cover - config è§£æå¤±è´¥æ—¶é‡‡ç”¨é»˜è®¤
        LOGGER.warning("æ— æ³•è¯»å– config.yaml: %s", exc)
    return {}


def _coerce_positive_int(value):
    if value is None:
        return None
    try:
        ivalue = int(str(value).strip())
    except (ValueError, TypeError):
        return None
    return ivalue if ivalue > 0 else None


def _resolve_int_setting(env_name: str, config_value, default: int) -> int:
    env_value = os.getenv(env_name)
    parsed_env = _coerce_positive_int(env_value)
    if parsed_env is not None:
        return parsed_env
    parsed_cfg = _coerce_positive_int(config_value)
    if parsed_cfg is not None:
        return parsed_cfg
    return default


_AGENT_CONFIG = _load_agent_config()
_UI_CONFIG = _AGENT_CONFIG.get("ui", {}) if isinstance(_AGENT_CONFIG, dict) else {}

_DEFAULT_LOG_MAX_CHARS = 4000
LOG_MAX_CHARS = _resolve_int_setting(
    "TOOL_LOG_MAX_CHARS",
    _UI_CONFIG.get("tool_log_max_chars"),
    _DEFAULT_LOG_MAX_CHARS,
)

_DEFAULT_STEP_SNIPPET = 500
LOG_STEP_SNIPPET_CHARS = _resolve_int_setting(
    "EXEC_LOG_SNIPPET_CHARS",
    _UI_CONFIG.get("execution_log_snippet_chars"),
    _DEFAULT_STEP_SNIPPET,
)

# ç”Ÿæˆå”¯ä¸€ä¼šè¯ID
SESSION_ID = os.getenv("AGENT_SESSION_ID", f"{MEMORY_CONFIG.session.default_session_prefix}_{uuid.uuid4().hex[:8]}")

# å¯ç”¨å¯¹è¯è®°å¿†çš„ Agent (ä½¿ç”¨é…ç½®) + Phase 4/5 ç”µå•†ä¼˜åŒ–
AGENT = LangChainAgent(
    use_memory=None,  # ä»é…ç½®è¯»å–
    session_id=SESSION_ID,
    persist_directory=None,  # ä»é…ç½®è¯»å–
    max_results=None,  # ä»é…ç½®è¯»å–
    use_similarity_search=None,  # ä»é…ç½®è¯»å–
    enable_conversation_state=True,  # Phase 4: å¯¹è¯çŠ¶æ€è·Ÿè¸ª
    enable_system_prompt=True,  # Phase 4: ç”µå•†ä¸“ç”¨æç¤ºè¯
    enable_quality_tracking=True,  # Phase 4: è´¨é‡è·Ÿè¸ª
    enable_intent_tracking=True,  # Phase 4: æ„å›¾è¯†åˆ«
    enable_recommendation=True,  # Phase 4: ä¸ªæ€§åŒ–æ¨è
)
LOGGER.info("ä¼šè¯ID: %s (åç«¯: %s, æ£€ç´¢æ¨¡å¼: %s)", 
           SESSION_ID, MEMORY_CONFIG.backend, MEMORY_CONFIG.strategy.retrieval_mode)

# å…¨å±€æ‰§è¡Œæ—¥å¿—å†å² - ä¿å­˜æ‰€æœ‰å¯¹è¯çš„æ‰§è¡Œæ—¥å¿—
EXECUTION_LOG_HISTORY = []
# å…¨å±€å¯¹è¯è®¡æ•°å™¨ - ç”¨äºé€’å¢ç¼–å·
CONVERSATION_COUNTER = 0
# å…¨å±€ Plan å†å²
PLAN_HISTORY = []
# å…¨å±€ Tool Call å†å²
TOOL_CALL_HISTORY = []


def _format_observation_for_ui(observation, *, max_chars: int = LOG_MAX_CHARS):
    """æ ¼å¼åŒ–å·¥å…·/æ—¥å¿—ç»“æœï¼Œå°½é‡ä¿ç•™å®Œæ•´ JSONã€‚"""

    def _truncate(text: str) -> str:
        if len(text) <= max_chars:
            return text
        return f"{text[:max_chars]}... (å·²æˆªæ–­)"

    if isinstance(observation, (dict, list)):
        pretty = json.dumps(observation, ensure_ascii=False, indent=2)
        return f"```json\n{_truncate(pretty)}\n```", True

    if isinstance(observation, str):
        stripped = observation.strip()
        if stripped.startswith("{") or stripped.startswith("["):
            try:
                parsed = json.loads(stripped)
                return _format_observation_for_ui(parsed, max_chars=max_chars)
            except (json.JSONDecodeError, TypeError):
                pass
        return _truncate(observation), False

    return _truncate(repr(observation)), False


def format_tool_log(log_entries) -> str:
    """æ ¼å¼åŒ–å·¥å…·è°ƒç”¨æ—¥å¿— - å•æ¬¡å¯¹è¯"""
    if not log_entries:
        return "(no tool calls yet)"
    lines = []
    for i, e in enumerate(log_entries, 1):
        tool_name = e.get('tool', '')
        
        # ğŸ¯ ä¸ºæœ¬ä½“æ¨ç†å·¥å…·æ·»åŠ ç‰¹æ®Šæ ‡è¯†
        if tool_name.startswith('ontology'):
            if 'validate' in tool_name:
                icon = "ğŸ›¡ï¸"  # SHACL æ ¡éªŒ
                tag = "[SHACLæ ¡éªŒ]"
            elif 'explain' in tool_name:
                icon = "ğŸ§ "  # æŠ˜æ‰£æ¨ç†è§£é‡Š
                tag = "[æœ¬ä½“æ¨ç†]"
            elif 'normalize' in tool_name:
                icon = "ğŸ”¤"  # å•†å“è§„èŒƒåŒ–
                tag = "[æœ¬ä½“æ¨ç†]"
            else:
                icon = "ğŸ¯"
                tag = "[æœ¬ä½“æ¨ç†]"
            lines.append(f"**#{i}** {icon} **{tag}** å·¥å…·: `{tool_name}`")
        else:
            lines.append(f"**#{i}** å·¥å…·: `{tool_name}`")
        
        lines.append(f"  - è¾“å…¥: `{e.get('input')}`")
        observation = e.get("observation")
        formatted_obs, is_block = _format_observation_for_ui(observation)
        if is_block:
            lines.append("  - è§‚å¯Ÿ:")
            lines.append(formatted_obs)
        else:
            lines.append(f"  - è§‚å¯Ÿ: {formatted_obs}")
        lines.append("")
    return "\n".join(lines)


def format_tool_log_history() -> str:
    """æ ¼å¼åŒ–ç´¯ç§¯çš„å·¥å…·è°ƒç”¨å†å² - æœ€æ–°çš„åœ¨é¡¶éƒ¨"""
    if not TOOL_CALL_HISTORY:
        return "## ğŸ”§ Tool Calls\n\n> *æš‚æ— å·¥å…·è°ƒç”¨è®°å½•*"
    
    lines = [f"## ğŸ”§ Tool Calls (å…± {len(TOOL_CALL_HISTORY)} æ¬¡å¯¹è¯)\n"]
    
    # ç»Ÿè®¡æœ¬ä½“æ¨ç†è°ƒç”¨
    ontology_count = 0
    shacl_count = 0
    for conversation in TOOL_CALL_HISTORY:
        for tool_call in conversation.get("tool_calls", []):
            tool_name = tool_call.get("tool", "")
            if tool_name.startswith("ontology"):
                ontology_count += 1
                if "validate" in tool_name:
                    shacl_count += 1
    
    if ontology_count > 0:
        lines.append(f"ğŸ¯ **æœ¬ä½“æ¨ç†è°ƒç”¨**: {ontology_count} æ¬¡ | ğŸ›¡ï¸ **SHACLæ ¡éªŒ**: {shacl_count} æ¬¡\n")
    
    # éå†å†å²(å·²æŒ‰æœ€æ–°åœ¨å‰æ’åº)
    for conversation in TOOL_CALL_HISTORY:
        conv_num = conversation.get("conversation_num", 0)
        conv_time = conversation.get("start_time", "")
        user_query = conversation.get("user_query", "")
        tool_calls = conversation.get("tool_calls", [])
        
        lines.append(f"### ğŸ”¹ å¯¹è¯ #{conv_num} - {conv_time}")
        lines.append(f"**æŸ¥è¯¢**: {user_query}")
        lines.append("")
        
        if not tool_calls:
            lines.append("> *æ­¤å¯¹è¯æœªè°ƒç”¨å·¥å…·*")
        else:
            for i, e in enumerate(tool_calls, 1):
                tool_name = e.get('tool', '')
                
                # ğŸ¯ ä¸ºæœ¬ä½“æ¨ç†å·¥å…·æ·»åŠ ç‰¹æ®Šæ ‡è¯†
                if tool_name.startswith('ontology'):
                    if 'validate' in tool_name:
                        icon = "ğŸ›¡ï¸"
                        tag = "[SHACLæ ¡éªŒ]"
                    elif 'explain' in tool_name:
                        icon = "ğŸ§ "
                        tag = "[æœ¬ä½“æ¨ç†]"
                    elif 'normalize' in tool_name:
                        icon = "ğŸ”¤"
                        tag = "[æœ¬ä½“æ¨ç†]"
                    else:
                        icon = "ğŸ¯"
                        tag = "[æœ¬ä½“æ¨ç†]"
                    lines.append(f"**#{i}** {icon} **{tag}** å·¥å…·: `{tool_name}`")
                else:
                    lines.append(f"**#{i}** å·¥å…·: `{tool_name}`")
                
                lines.append(f"  - è¾“å…¥: `{e.get('input')}`")
                observation = e.get("observation")
                formatted_obs, is_block = _format_observation_for_ui(observation)
                if is_block:
                    lines.append("  - è§‚å¯Ÿ:")
                    lines.append(formatted_obs)
                else:
                    lines.append(f"  - è§‚å¯Ÿ: {formatted_obs}")
                lines.append("")
        
        lines.append("---\n")  # å¯¹è¯é—´åˆ†éš”çº¿
    
    return "\n".join(lines)


def format_plan_history() -> str:
    """æ ¼å¼åŒ–ç´¯ç§¯çš„è®¡åˆ’å†å² - æœ€æ–°çš„åœ¨é¡¶éƒ¨"""
    if not PLAN_HISTORY:
        return "## ğŸ“‹ Plan / Tasks\n\n> *æš‚æ— è®¡åˆ’è®°å½•*"
    
    lines = [f"## ğŸ“‹ Plan / Tasks (å…± {len(PLAN_HISTORY)} æ¬¡å¯¹è¯)\n"]
    
    # éå†å†å²(å·²æŒ‰æœ€æ–°åœ¨å‰æ’åº)
    for conversation in PLAN_HISTORY:
        conv_num = conversation.get("conversation_num", 0)
        conv_time = conversation.get("start_time", "")
        user_query = conversation.get("user_query", "")
        plan = conversation.get("plan", "")
        
        lines.append(f"### ğŸ”¹ å¯¹è¯ #{conv_num} - {conv_time}")
        lines.append(f"**æŸ¥è¯¢**: {user_query}")
        lines.append("")
        
        if not plan or plan == "(no plan provided)":
            lines.append("> *æ­¤å¯¹è¯æ— è®¡åˆ’*")
        else:
            lines.append(plan)
        
        lines.append("")
        lines.append("---\n")  # å¯¹è¯é—´åˆ†éš”çº¿
    
    return "\n".join(lines)


def format_execution_log(execution_log) -> str:
    """æ ¼å¼åŒ–è¯¦ç»†æ‰§è¡Œæ—¥å¿— - æ˜¾ç¤ºæ‰€æœ‰è¾“å…¥è¾“å‡ºçš„å®Œæ•´ç»“æ„"""
    if not execution_log:
        return "## è¿è¡Œæ—¥å¿—\n\n(æš‚æ— æ—¥å¿—)"
    
    lines = [f"## è¿è¡Œæ—¥å¿— ({len(execution_log)} æ¡è®°å½•)\n"]
    
    for i, entry in enumerate(execution_log, 1):
        step_type = entry.get("step_type", "unknown")
        content = entry.get("content", "")
        metadata = entry.get("metadata", {})
        timestamp = entry.get("timestamp", "")
        
        # æ ¹æ®æ­¥éª¤ç±»å‹æ ¼å¼åŒ–
        if step_type == "user_input":
            lines.append(f"**[{i}] ğŸ“ ç”¨æˆ·è¾“å…¥**")
            lines.append(f"```text\n{content}\n```\n")
            
        elif step_type == "memory_retrieval":
            lines.append(f"**[{i}] ğŸ§  è®°å¿†æ£€ç´¢**")
            lines.append(f"- æ¨¡å¼: `{metadata.get('mode', 'unknown')}`")
            lines.append(f"- ç»“æœé•¿åº¦: {metadata.get('result_length', 0)} å­—ç¬¦\n")
            
        elif step_type == "memory_context":
            lines.append(f"**[{i}] ğŸ“š å†å²ä¸Šä¸‹æ–‡æ³¨å…¥**")
            lines.append(f"```text\n{content}\n```\n")
            
        elif step_type == "enhanced_prompt":
            lines.append(f"**[{i}] ğŸ¯ å¢å¼ºæç¤ºè¯**")
            has_ctx = metadata.get("has_context", False)
            lines.append(f"- åŒ…å«å†å²ä¸Šä¸‹æ–‡: {'æ˜¯' if has_ctx else 'å¦'}")
            if has_ctx:
                lines.append(f"- ä¸Šä¸‹æ–‡é•¿åº¦: {metadata.get('context_length', 0)} å­—ç¬¦")
            lines.append(f"```text\n{content}\n```\n")
            
        elif step_type == "iteration_start":
            iteration = metadata.get("iteration", 0)
            lines.append(f"**[{i}] ğŸ”„ æ¨ç†è½®æ¬¡ {iteration}**\n")
            
        elif step_type == "llm_input":
            iteration = metadata.get("iteration", 0)
            llm_class = metadata.get("llm_class", "Unknown")
            llm_module = metadata.get("llm_module", "Unknown")
            llm_method = metadata.get("llm_method", "generate")
            
            lines.append(f"**[{i}] ğŸ“¤ LLM è¾“å…¥ (ç¬¬ {iteration} è½®)**")
            lines.append(f"- LLM ç±»: `{llm_module}.{llm_class}`")
            lines.append(f"- è°ƒç”¨æ–¹æ³•: `{llm_method}()`")
            
            if isinstance(content, dict):
                messages = content.get("messages", [])
                tools = content.get("tools", [])
                
                lines.append(f"- æ¶ˆæ¯æ•°é‡: {len(messages)}")
                lines.append(f"- å¯ç”¨å·¥å…·æ•°: {len(tools)}")
                
                # æ˜¾ç¤ºå·¥å…·åˆ—è¡¨ - ä¿®æ­£å·¥å…·åç§°æå–
                if tools:
                    tool_names = []
                    for t in tools:
                        # OpenAI å‡½æ•°è°ƒç”¨æ ¼å¼: {"type": "function", "function": {"name": "...", ...}}
                        if isinstance(t, dict):
                            if "function" in t and isinstance(t["function"], dict):
                                tool_names.append(t["function"].get("name", "unknown"))
                            else:
                                tool_names.append(t.get("name", "unknown"))
                        else:
                            tool_names.append("unknown")
                    lines.append(f"- å·¥å…·åˆ—è¡¨: `{', '.join(tool_names)}`")
                
                # æ˜¾ç¤ºæ‰€æœ‰æ¶ˆæ¯
                lines.append(f"\n**æ¶ˆæ¯åˆ—è¡¨**:")
                for idx, msg in enumerate(messages, 1):
                    role = msg.get("role", "unknown")
                    msg_content = msg.get("content", "")
                    lines.append(f"\næ¶ˆæ¯ {idx} [{role}]:")
                    if msg_content:
                        lines.append(f"```\n{msg_content}\n```")
                    # å¦‚æœæœ‰ tool_calls
                    if "tool_calls" in msg:
                        lines.append(f"å·¥å…·è°ƒç”¨: {len(msg['tool_calls'])} ä¸ª")
                
                # æ˜¾ç¤ºå·¥å…·å®šä¹‰
                if tools:
                    lines.append(f"\n**å·¥å…·å®šä¹‰**:")
                    for tool in tools:
                        lines.append(f"```json\n{json.dumps(tool, ensure_ascii=False, indent=2)}\n```")
            else:
                lines.append(f"```\n{content}\n```")
            lines.append("")
                    
        elif step_type == "llm_output":
            iteration = metadata.get("iteration", 0)
            lines.append(f"**[{i}] ğŸ“¥ LLM è¾“å‡º (ç¬¬ {iteration} è½®)**")
            
            if isinstance(content, dict):
                response_text = content.get("content", "")
                tool_calls_count = content.get("tool_calls_count", 0)
                tool_calls = content.get("tool_calls", [])
                
                lines.append(f"- å·¥å…·è°ƒç”¨æ•°: {tool_calls_count}")
                
                if response_text:
                    lines.append(f"\n**æ¨¡å‹å“åº”æ–‡æœ¬**:")
                    lines.append(f"```\n{response_text}\n```")
                
                if tool_calls:
                    lines.append(f"\n**æ¨¡å‹è¯·æ±‚çš„å·¥å…·è°ƒç”¨**:")
                    for tc in tool_calls:
                        lines.append(f"```json\n{json.dumps(tc, ensure_ascii=False, indent=2)}\n```")
            else:
                lines.append(f"```\n{content}\n```")
            lines.append("")
                
        elif step_type == "tool_call":
            iteration = metadata.get("iteration", 0)
            tool_class = metadata.get("tool_class", "Unknown")
            tool_module = metadata.get("tool_module", "Unknown")
            
            if isinstance(content, dict):
                tool_name = content.get("name", "unknown")
                args = content.get("arguments", {})
                class_info = content.get("class_info", {})
                
                # ğŸ¯ ä¸ºæœ¬ä½“æ¨ç†å·¥å…·æ·»åŠ ç‰¹æ®Šæ ‡è¯†
                if tool_name.startswith('ontology'):
                    if 'validate' in tool_name:
                        icon = "ğŸ›¡ï¸"
                        tag = "[SHACLæ ¡éªŒ]"
                    elif 'explain' in tool_name:
                        icon = "ğŸ§ "
                        tag = "[æœ¬ä½“æ¨ç†]"
                    elif 'normalize' in tool_name:
                        icon = "ğŸ”¤"
                        tag = "[æœ¬ä½“æ¨ç†]"
                    else:
                        icon = "ğŸ¯"
                        tag = "[æœ¬ä½“æ¨ç†]"
                    lines.append(f"**[{i}] {icon} **{tag}** æ‰§è¡Œå·¥å…·è°ƒç”¨ (ç¬¬ {iteration} è½®)**")
                else:
                    lines.append(f"**[{i}] ğŸ”§ æ‰§è¡Œå·¥å…·è°ƒç”¨ (ç¬¬ {iteration} è½®)**")
                
                lines.append(f"- å·¥å…·åç§°: `{tool_name}`")
                lines.append(f"- å·¥å…·ç±»: `{class_info.get('module', tool_module)}.{class_info.get('class', tool_class)}`")
                lines.append(f"- è°ƒç”¨æ–¹æ³•: `invoke()`")
                lines.append(f"\n**è°ƒç”¨å‚æ•°**:")
                lines.append(f"```json\n{json.dumps(args, ensure_ascii=False, indent=2)}\n```\n")
            else:
                lines.append(f"**[{i}] ğŸ”§ æ‰§è¡Œå·¥å…·è°ƒç”¨ (ç¬¬ {iteration} è½®)**")
                lines.append(f"```\n{content}\n```\n")
                
        elif step_type == "tool_result":
            iteration = metadata.get("iteration", 0)
            tool_name = metadata.get("tool_name", "unknown")
            invoked_class = metadata.get("invoked_class", "Unknown")
            invoked_module = metadata.get("invoked_module", "Unknown")
            invoked_method = metadata.get("invoked_method", "invoke")
            
            # ğŸ¯ ä¸ºæœ¬ä½“æ¨ç†å·¥å…·ç»“æœæ·»åŠ ç‰¹æ®Šæ ‡è¯†
            if tool_name.startswith('ontology'):
                if 'validate' in tool_name:
                    icon = "ğŸ›¡ï¸"
                    tag = "[SHACLæ ¡éªŒ]"
                elif 'explain' in tool_name:
                    icon = "ğŸ§ "
                    tag = "[æœ¬ä½“æ¨ç†]"
                elif 'normalize' in tool_name:
                    icon = "ğŸ”¤"
                    tag = "[æœ¬ä½“æ¨ç†]"
                else:
                    icon = "ğŸ¯"
                    tag = "[æœ¬ä½“æ¨ç†]"
                lines.append(f"**[{i}] {icon} **{tag}** å·¥å…·æ‰§è¡Œç»“æœ (ç¬¬ {iteration} è½®)**")
            else:
                lines.append(f"**[{i}] âœ… å·¥å…·æ‰§è¡Œç»“æœ (ç¬¬ {iteration} è½®)**")
            
            lines.append(f"- å·¥å…·åç§°: `{tool_name}`")
            lines.append(f"- æ‰§è¡Œç±»: `{invoked_module}.{invoked_class}`")
            lines.append(f"- æ‰§è¡Œæ–¹æ³•: `{invoked_method}()`")
            lines.append(f"\n**è¿”å›ç»“æœ**:")
            
            # å°è¯•è§£æ JSON
            try:
                if isinstance(content, str):
                    parsed = json.loads(content)
                    lines.append(f"```json\n{json.dumps(parsed, ensure_ascii=False, indent=2)}\n```\n")
                else:
                    lines.append(f"```json\n{json.dumps(content, ensure_ascii=False, indent=2)}\n```\n")
            except (json.JSONDecodeError, TypeError):
                lines.append(f"```text\n{content}\n```\n")
                
        elif step_type == "final_answer":
            iteration = metadata.get("iteration", 0)
            lines.append(f"**[{i}] ğŸ‰ æœ€ç»ˆç­”æ¡ˆ (ç¬¬ {iteration} è½®)**")
            lines.append(f"```text\n{content}\n```\n")
            
        elif step_type == "memory_save":
            lines.append(f"**[{i}] ğŸ’¾ ä¿å­˜å¯¹è¯è®°å¿†**")
            lines.append(f"- ç”¨æˆ·è¾“å…¥: {metadata.get('user_input_length', 0)} å­—ç¬¦")
            lines.append(f"- åŠ©æ‰‹å“åº”: {metadata.get('response_length', 0)} å­—ç¬¦")
            lines.append(f"- å·¥å…·è°ƒç”¨æ•°: {metadata.get('tool_calls_count', 0)}\n")
            
        elif step_type == "memory_saved":
            lines.append(f"**[{i}] âœ… è®°å¿†ä¿å­˜æˆåŠŸ**")
            lines.append(f"- {content}\n")
            
        elif step_type == "execution_complete":
            lines.append(f"**[{i}] ğŸ æ‰§è¡Œå®Œæˆ**")
            lines.append(f"- æ€»è¿­ä»£æ¬¡æ•°: {metadata.get('iterations_used', 0)}")
            lines.append(f"- å·¥å…·è°ƒç”¨æ¬¡æ•°: {metadata.get('tool_calls', 0)}\n")
            
        elif step_type == "max_iterations":
            lines.append(f"**[{i}] âš ï¸ è¾¾åˆ°æœ€å¤§è¿­ä»£é™åˆ¶**")
            lines.append(f"- é™åˆ¶æ¬¡æ•°: {metadata.get('max_iterations', 0)}\n")
            
        elif step_type == "ontology_inference":
            lines.append(f"**[{i}] ğŸ§  æœ¬ä½“æ¨ç†**")
            if isinstance(content, dict):
                inference_type = content.get("inference_type", "unknown")
                lines.append(f"- æ¨ç†ç±»å‹: `{inference_type}`")
                lines.append(f"- è®¢å•ID: {content.get('order_id')}")
                if content.get("order_no"):
                    lines.append(f"- è®¢å•å·: {content.get('order_no')}")
                lines.append(f"- è®¢å•çŠ¶æ€: {content.get('order_status')}")
                lines.append(f"- ä¸‹å•åæ—¶é•¿: {content.get('hours_since_created')} å°æ—¶")
                lines.append(f"- æ˜¯å¦å·²æœ‰ç‰©æµ: {'æ˜¯' if content.get('has_shipment') else 'å¦'}")
                policy = content.get("policy")
                if policy:
                    lines.append("\n**æ¨ç†ç»“æœ**:")
                    lines.append(f"```json\n{json.dumps(policy, ensure_ascii=False, indent=2)}\n```")
            else:
                lines.append(f"```text\n{content}\n```")
            if metadata:
                lines.append(f"- æ¥æº: {metadata.get('source', 'unknown')}")
                if metadata.get("ontology_method"):
                    lines.append(f"- æ–¹æ³•: {metadata['ontology_method']}")
            lines.append("")

        elif step_type == "llm_error":
            llm_class = metadata.get("llm_class", "Unknown")
            llm_module = metadata.get("llm_module", "Unknown")
            lines.append(f"**[{i}] âŒ LLM è°ƒç”¨é”™è¯¯**")
            lines.append(f"- LLM ç±»: `{llm_module}.{llm_class}`")
            lines.append(f"- é”™è¯¯ç±»å‹: `{metadata.get('error_type', 'Unknown')}`")
            lines.append(f"- é”™è¯¯ä¿¡æ¯: {metadata.get('error_message', 'No details')}")
            lines.append(f"```\n{content}\n```\n")
        
        else:
            # æœªçŸ¥ç±»å‹,æ˜¾ç¤ºåŸå§‹å†…å®¹
            lines.append(f"**[{i}] {step_type}**")
            lines.append(f"```\n{content}\n```\n")
        
        # æ—¶é—´æˆ³(å°å­—ä½“)
        lines.append(f"<sub>â±ï¸ {timestamp}</sub>\n")
    
    return "\n".join(lines)


def format_execution_log_history() -> str:
    """æ ¼å¼åŒ–ç´¯ç§¯çš„æ‰§è¡Œæ—¥å¿—å†å² - æœ€æ–°çš„åœ¨é¡¶éƒ¨"""
    if not EXECUTION_LOG_HISTORY:
        return "## ğŸ“‹ æ‰§è¡Œæ—¥å¿—å†å²\n\n> *æš‚æ— æ‰§è¡Œè®°å½•*"
    
    lines = [f"## ğŸ“‹ æ‰§è¡Œæ—¥å¿—å†å² (å…± {len(EXECUTION_LOG_HISTORY)} æ¬¡å¯¹è¯)\n"]
    
    # éå†å†å²(å·²æŒ‰æœ€æ–°åœ¨å‰æ’åº,ä½¿ç”¨å­˜å‚¨çš„å¯¹è¯ç¼–å·)
    for conversation in EXECUTION_LOG_HISTORY:
        conv_num = conversation.get("conversation_num", 0)  # ä½¿ç”¨å­˜å‚¨çš„ç¼–å·
        conv_logs = conversation.get("logs", [])
        conv_time = conversation.get("start_time", "")
        user_query = conversation.get("user_query", "")
        
        # å¯¹è¯æ ‡é¢˜
        lines.append(f"### ğŸ”¹ å¯¹è¯ #{conv_num} - {conv_time}")
        lines.append(f"**æŸ¥è¯¢**: {user_query}")
        lines.append("")
        
        # æ˜¾ç¤ºè¯¥å¯¹è¯çš„æ‰€æœ‰æ‰§è¡Œæ­¥éª¤
        for i, entry in enumerate(conv_logs, 1):
            step_type = entry.get("step_type", "unknown")
            timestamp = entry.get("timestamp", "")
            content = entry.get("content", "")
            metadata = entry.get("metadata", {})
            
            # è·å–å›¾æ ‡
            icon_map = {
                "user_input": "ğŸ“",
                "memory_retrieval": "ğŸ§ ",
                "memory_context": "ğŸ“š",
                "enhanced_prompt": "ğŸ¯",
                "iteration_start": "ğŸ”„",
                "llm_input": "ğŸ“¤",
                "llm_output": "ğŸ“¥",
                "tool_call": "ğŸ”§",
                "tool_result": "âœ…",
                "final_answer": "ğŸ‰",
                "memory_save": "ğŸ’¾",
                "memory_saved": "âœ…",
                "execution_complete": "ğŸ",
                "llm_error": "âŒ",
                "max_iterations": "âš ï¸",
                "ontology_inference": "ğŸ§ ",
            }
            icon = icon_map.get(step_type, "ğŸ“„")
            
            # æ­¥éª¤æ ‡é¢˜
            time_short = timestamp.split('T')[1][:8] if 'T' in timestamp else timestamp
            lines.append(f"**æ­¥éª¤ {i}**: {icon} `{step_type}` <sub>{time_short}</sub>")
            
            # æ˜¾ç¤ºå…³é”®æ•°æ®
            if step_type == "user_input":
                lines.append(f"  - ç”¨æˆ·è¾“å…¥: {content}")
            
            elif step_type == "memory_retrieval":
                lines.append(f"  - æ£€ç´¢è¯´æ˜: {content}")
                if metadata:
                    mode = metadata.get('mode', '')
                    length = metadata.get('result_length', 0)
                    lines.append(f"  - æ¨¡å¼: `{mode}`, ç»“æœé•¿åº¦: {length}")
            
            elif step_type == "llm_input":
                if isinstance(content, dict):
                    messages = content.get('messages', [])
                    tools = content.get('tools', [])
                    llm_class = metadata.get('llm_class', 'Unknown')
                    llm_module = metadata.get('llm_module', 'Unknown')
                    
                    lines.append(f"  - LLM ç±»: `{llm_module}.{llm_class}`")
                    lines.append(f"  - æ¶ˆæ¯æ•°: {len(messages)}, å·¥å…·æ•°: {len(tools)}")
                    if tools:
                        # æå–å·¥å…·åç§°
                        tool_names = []
                        for t in tools:
                            if isinstance(t, dict):
                                if "function" in t and isinstance(t["function"], dict):
                                    tool_names.append(t["function"].get("name", "unknown"))
                                else:
                                    tool_names.append(t.get("name", "unknown"))
                        lines.append(f"  - å·¥å…·åˆ—è¡¨: {', '.join(tool_names)}")
            
            elif step_type == "llm_output":
                if isinstance(content, dict):
                    text = content.get('content', '')
                    tool_calls = content.get('tool_calls', [])
                    snippet = LOG_STEP_SNIPPET_CHARS
                    lines.append(
                        f"  - å›å¤: {text[:snippet]}..." if len(text) > snippet else f"  - å›å¤: {text}"
                    )
                    if tool_calls:
                        call_names = []
                        for tc in tool_calls:
                            if isinstance(tc, dict):
                                # tool_calls æ ¼å¼: [{"id": "...", "name": "...", "arguments": {...}}]
                                name = tc.get('name', 'unknown')
                                call_names.append(name)
                        lines.append(f"  - è°ƒç”¨å·¥å…·: {', '.join(call_names)}")
                else:
                    snippet = LOG_STEP_SNIPPET_CHARS
                    content_str = str(content)
                    lines.append(
                        f"  - å›å¤: {content_str[:snippet]}..." if len(content_str) > snippet else f"  - å›å¤: {content_str}"
                    )
            
            elif step_type == "tool_call":
                if metadata:
                    tool_class = metadata.get('tool_class', '')
                    tool_module = metadata.get('tool_module', '')
                    lines.append(f"  - å·¥å…·ç±»: `{tool_module}.{tool_class}`")
                if isinstance(content, dict):
                    tool_name = content.get("name", "unknown")
                    
                    # ğŸ¯ ä¸ºæœ¬ä½“æ¨ç†å·¥å…·æ·»åŠ ç‰¹æ®Šæ ‡è¯†
                    if tool_name.startswith('ontology'):
                        if 'validate' in tool_name:
                            icon = "ğŸ›¡ï¸"
                            tag = "[SHACLæ ¡éªŒ]"
                        elif 'explain' in tool_name:
                            icon = "ğŸ§ "
                            tag = "[æœ¬ä½“æ¨ç†]"
                        elif 'normalize' in tool_name:
                            icon = "ğŸ”¤"
                            tag = "[æœ¬ä½“æ¨ç†]"
                        else:
                            icon = "ğŸ¯"
                            tag = "[æœ¬ä½“æ¨ç†]"
                        lines.append(f"  {icon} **{tag}** å·¥å…·å: `{tool_name}`")
                    else:
                        lines.append(f"  - å·¥å…·å: `{tool_name}`")
                    
                    snippet = LOG_STEP_SNIPPET_CHARS
                    args_str_full = str(content.get("arguments", {}))
                    args_str = args_str_full[:snippet]
                    lines.append(
                        f"  - å‚æ•°: {args_str}..." if len(args_str_full) > snippet else f"  - å‚æ•°: {args_str}"
                    )
            
            elif step_type == "tool_result":
                if metadata:
                    tool_name = metadata.get('tool_name', '')
                    invoked_class = metadata.get('invoked_class', '')
                    invoked_module = metadata.get('invoked_module', '')
                    
                    # ğŸ¯ ä¸ºæœ¬ä½“æ¨ç†å·¥å…·ç»“æœæ·»åŠ ç‰¹æ®Šæ ‡è¯†
                    if tool_name.startswith('ontology'):
                        if 'validate' in tool_name:
                            icon = "ğŸ›¡ï¸"
                            tag = "[SHACLæ ¡éªŒ]"
                        elif 'explain' in tool_name:
                            icon = "ğŸ§ "
                            tag = "[æœ¬ä½“æ¨ç†]"
                        elif 'normalize' in tool_name:
                            icon = "ğŸ”¤"
                            tag = "[æœ¬ä½“æ¨ç†]"
                        else:
                            icon = "ğŸ¯"
                            tag = "[æœ¬ä½“æ¨ç†]"
                        lines.append(f"  {icon} **{tag}** å·¥å…·: `{tool_name}`")
                    else:
                        lines.append(f"  - å·¥å…·: `{tool_name}`")
                    
                    lines.append(f"  - æ‰§è¡Œç±»: `{invoked_module}.{invoked_class}`")
                formatted_result, is_block = _format_observation_for_ui(content)
                if is_block:
                    lines.append("  - ç»“æœ:")
                    lines.append(formatted_result)
                else:
                    lines.append(f"  - ç»“æœ: {formatted_result}")
            
            elif step_type == "final_answer":
                snippet = LOG_STEP_SNIPPET_CHARS
                content_str = str(content)
                lines.append(
                    f"  - æœ€ç»ˆå›ç­”: {content_str[:snippet]}..." if len(content_str) > snippet else f"  - æœ€ç»ˆå›ç­”: {content_str}"
                )
            
            elif step_type == "llm_error":
                lines.append(f"  - âŒ é”™è¯¯: {content}")

            elif step_type == "ontology_inference":
                if isinstance(content, dict):
                    inference_type = content.get('inference_type', 'unknown')
                    lines.append(f"  - æ¨ç†ç±»å‹: `{inference_type}`")
                    lines.append(f"  - è®¢å•ID: {content.get('order_id')}")
                    if content.get('order_no'):
                        lines.append(f"  - è®¢å•å·: {content.get('order_no')}")
                    lines.append(f"  - çŠ¶æ€: {content.get('order_status')}")
                    lines.append(f"  - è·ä¸‹å•: {content.get('hours_since_created')} å°æ—¶")
                    lines.append(f"  - æœ‰ç‰©æµ: {'æ˜¯' if content.get('has_shipment') else 'å¦'}")
                    policy = content.get('policy')
                    if policy:
                        snippet = LOG_STEP_SNIPPET_CHARS
                        policy_str = json.dumps(policy, ensure_ascii=False)
                        lines.append(
                            f"  - æ¨ç†ç»“æœ: {policy_str[:snippet]}..." if len(policy_str) > snippet else f"  - æ¨ç†ç»“æœ: {policy_str}"
                        )
                else:
                    lines.append(f"  - å†…å®¹: {content}")
                if metadata:
                    lines.append(f"  - æ¥æº: {metadata.get('source', 'unknown')}")
                    if metadata.get('ontology_method'):
                        lines.append(f"  - æ–¹æ³•: {metadata['ontology_method']}")
            
            lines.append("")
        
        lines.append("---\n")  # å¯¹è¯é—´åˆ†éš”çº¿
    
    return "\n".join(lines)


def format_memory_context() -> str:
    """æ ¼å¼åŒ–å¯¹è¯è®°å¿†ä¸Šä¸‹æ–‡(Markdown)"""
    context = AGENT.get_memory_context()
    stats = AGENT.get_memory_stats()
    config = MEMORY_CONFIG
    
    if not context:
        lines = []
        lines.append(f"**åç«¯**: `{stats.get('backend', 'Unknown')}`  ")
        lines.append(f"**æ£€ç´¢æ¨¡å¼**: `{config.strategy.retrieval_mode}`  ")
        if 'persist_directory' in stats:
            persist_dir = stats['persist_directory']
            # åªæ˜¾ç¤ºæœ€åä¸¤çº§ç›®å½•
            short_dir = '/'.join(persist_dir.split('/')[-2:])
            lines.append(f"**å­˜å‚¨**: `.../{short_dir}`  ")
        lines.append(f"**ä¼šè¯**: `{stats.get('session_id', 'N/A')[:12]}...`  ")
        lines.append(f"**è®°å½•æ•°**: `{stats.get('total_turns', 0)}`  ")
        max_results = config.strategy.max_recent_turns if config.strategy.retrieval_mode == 'recent' else config.strategy.max_similarity_results
        lines.append(f"**æœ€å¤§ç»“æœ**: `{max_results}`  ")
        lines.append("")
        lines.append("> *æš‚æ— å†å²è®°å½•*")
        return "\n".join(lines)
    
    lines = []
    lines.append(f"**åç«¯**: `{stats.get('backend', 'Unknown')}` | **æ¨¡å¼**: `{config.strategy.retrieval_mode}` | **è®°å½•æ•°**: `{stats.get('total_turns', 0)}`")
    lines.append("")
    lines.append("---")
    lines.append("")
    lines.append(context)
    
    return "\n".join(lines)


def format_ecommerce_analysis() -> str:
    """æ ¼å¼åŒ–ç”µå•†åˆ†ææ•°æ®ï¼ˆè´¨é‡ã€æ„å›¾ã€æ¨èï¼‰"""
    lines = ["## ğŸ›ï¸ ç”µå•†æ™ºèƒ½åˆ†æ\n"]
    
    # 1. å¯¹è¯è´¨é‡è¯„åˆ†
    quality_report = AGENT.get_quality_report()
    if quality_report:
        lines.append("### ğŸ“Š å¯¹è¯è´¨é‡è¯„åˆ†\n")
        score = quality_report.get('quality_score', 0)
        
        # è¯„çº§
        if score >= 80:
            grade = "ğŸ† ä¼˜ç§€"
        elif score >= 60:
            grade = "âœ… è‰¯å¥½"
        elif score >= 40:
            grade = "âš ï¸ åŠæ ¼"
        else:
            grade = "âŒ å¾…æ”¹è¿›"
        
        lines.append(f"**ç»¼åˆè¯„åˆ†**: {score}/100 {grade}\n")
        
        efficiency = quality_report.get('efficiency', {})
        completion = quality_report.get('task_completion', {})
        conv_quality = quality_report.get('conversation_quality', {})
        
        lines.append("| æŒ‡æ ‡ | æ•°å€¼ |")
        lines.append("|------|------|")
        lines.append(f"| å¹³å‡å“åº”æ—¶é—´ | {efficiency.get('avg_response_time', 0):.2f}ç§’ |")
        lines.append(f"| å¹³å‡å·¥å…·è°ƒç”¨ | {efficiency.get('avg_tool_calls', 0):.2f}æ¬¡ |")
        lines.append(f"| ä»»åŠ¡æˆåŠŸç‡ | {completion.get('success_rate', 0)*100:.1f}% |")
        lines.append(f"| æ¾„æ¸…ç‡ | {conv_quality.get('clarification_rate', 0)*100:.1f}% |")
        lines.append(f"| ä¸»åŠ¨å¼•å¯¼ç‡ | {conv_quality.get('proactive_rate', 0)*100:.1f}% |")
        lines.append("")
    
    # 2. æ„å›¾åˆ†æ
    intent_analysis = AGENT.get_intent_analysis()
    if intent_analysis and intent_analysis.get('total_turns', 0) > 0:
        lines.append("### ğŸ¯ æ„å›¾åˆ†æ\n")
        
        lines.append(f"**æ€»è½®æ¬¡**: {intent_analysis.get('total_turns', 0)}\n")
        
        # æ„å›¾åˆ†å¸ƒ
        intent_dist = intent_analysis.get('intent_distribution', {})
        if intent_dist:
            lines.append("**æ„å›¾åˆ†å¸ƒ**:\n")
            for intent_type, count in sorted(intent_dist.items(), key=lambda x: x[1], reverse=True):
                percentage = (count / intent_analysis['total_turns'] * 100) if intent_analysis['total_turns'] > 0 else 0
                bar = "â–ˆ" * int(percentage / 5)  # æ¯5%ä¸€ä¸ªæ–¹å—
                lines.append(f"- `{intent_type}`: {bar} {count}æ¬¡ ({percentage:.0f}%)")
            lines.append("")
        
        # å¤åˆæ„å›¾
        composite = intent_analysis.get('composite_intents', [])
        if composite:
            lines.append("**å¤åˆæ„å›¾**:\n")
            for comp in composite:
                name = comp.get('name', 'Unknown')
                desc = comp.get('description', '')
                confidence = comp.get('confidence', 0)
                
                if name == "purchase_intent":
                    icon = "ğŸ’°"
                elif name == "comparison_intent":
                    icon = "ğŸ”„"
                elif name == "after_sales_intent":
                    icon = "ğŸ“"
                else:
                    icon = "ğŸ“"
                
                lines.append(f"- {icon} **{name}**: {desc} (ç½®ä¿¡åº¦: {confidence:.2f})")
            lines.append("")
        
        # å½“å‰çŠ¶æ€
        current = intent_analysis.get('current_intent', '')
        predicted = intent_analysis.get('predicted_next', [])
        if current:
            lines.append(f"**å½“å‰æ„å›¾**: `{current}`")
        if predicted:
            lines.append(f"**é¢„æµ‹ä¸‹ä¸€æ­¥**: `{', '.join(predicted)}`")
        lines.append("")
    
    # 3. å¯¹è¯çŠ¶æ€
    if hasattr(AGENT, 'state_manager') and AGENT.state_manager and AGENT.state_manager.state:
        lines.append("### ğŸ›’ å¯¹è¯çŠ¶æ€\n")
        state = AGENT.state_manager.state
        
        stage_emoji = {
            "greeting": "ğŸ‘‹",
            "browsing": "ğŸ”",
            "selecting": "ğŸ‘€",
            "cart": "ğŸ›’",
            "checkout": "ğŸ’³",
            "tracking": "ğŸ“¦",
            "service": "ğŸ’¬",
            "idle": "ğŸ’¤"
        }
        
        stage = state.stage.value
        lines.append(f"**å½“å‰é˜¶æ®µ**: {stage_emoji.get(stage, 'ğŸ“')} {stage}\n")
        
        # ç”¨æˆ·ä¸Šä¸‹æ–‡
        user_ctx = state.user_context
        lines.append("**ç”¨æˆ·ä¿¡æ¯**:")
        lines.append(f"- ç”¨æˆ·ID: {user_ctx.user_id or 'æœªç™»å½•'}")
        lines.append(f"- VIPçŠ¶æ€: {'æ˜¯ â­' if user_ctx.is_vip else 'å¦'}")
        lines.append(f"- è´­ç‰©è½¦: {user_ctx.cart_item_count} ä»¶")
        
        if user_ctx.last_viewed_products:
            viewed = ', '.join(str(p) for p in user_ctx.last_viewed_products[:3])
            lines.append(f"- æœ€è¿‘æµè§ˆ: {viewed}")
        
        if user_ctx.recent_order_id:
            lines.append(f"- æœ€è¿‘è®¢å•: {user_ctx.recent_order_id}")
        
        lines.append("")
    
    # å¦‚æœæ²¡æœ‰ä»»ä½•æ•°æ®
    if len(lines) == 1:
        lines.append("> *å¼€å§‹å¯¹è¯åæ˜¾ç¤ºåˆ†ææ•°æ®*")
    
    return "\n".join(lines)


def _normalize_chatbot_messages(history):
    normalized = []
    if not history:
        return normalized
    for entry in history:
        if isinstance(entry, dict):
            role = entry.get("role") or ("assistant" if normalized and normalized[-1]["role"] == "user" else "user")
            content = entry.get("content")
            normalized.append({
                "role": role,
                "content": "" if content is None else str(content),
            })
        elif isinstance(entry, (list, tuple)) and len(entry) == 2:
            user_msg, assistant_msg = entry
            if user_msg is not None:
                normalized.append({"role": "user", "content": str(user_msg)})
            if assistant_msg is not None:
                normalized.append({"role": "assistant", "content": str(assistant_msg)})
        else:
            continue
    return normalized


def handle_user_message(user_message, chat_history=None):
    """å¤„ç†ç”¨æˆ·æ¶ˆæ¯å¹¶æ›´æ–° UI"""
    from datetime import datetime
    global CONVERSATION_COUNTER, PLAN_HISTORY, TOOL_CALL_HISTORY
    
    chat_history = _normalize_chatbot_messages(chat_history)
    chat_history.append({"role": "user", "content": user_message})
    assistant_placeholder = {"role": "assistant", "content": ""}
    chat_history.append(assistant_placeholder)

    LOGGER.info("Gradio incoming: %s", user_message[:200])
    
    try:
        res = AGENT.run(user_message)
        final = res.get("final_answer") or ""
        plan = res.get("plan") or "(no plan provided)"
        tool_calls = res.get("tool_log", [])
        
        # Phase 4/5: è·å–ç”µå•†å¢å¼ºä¿¡æ¯
        intent_summary = res.get("intent_summary", {})
        quality_metrics = res.get("quality_metrics", {})
        conv_state = res.get("conversation_state", {})
        
        # åœ¨å›å¤ä¸­æ·»åŠ ç”µå•†ä¸Šä¸‹æ–‡æç¤ºï¼ˆå¦‚æœæœ‰ï¼‰
        ecommerce_context = []
        
        # 1. æ˜¾ç¤ºå½“å‰è´­ç‰©é˜¶æ®µ
        if conv_state:
            stage = conv_state.get("stage", "unknown")
            stage_emoji = {
                "greeting": "ğŸ‘‹",
                "browsing": "ğŸ”",
                "selecting": "ğŸ‘€",
                "cart": "ğŸ›’",
                "checkout": "ğŸ’³",
                "tracking": "ğŸ“¦",
                "service": "ğŸ’¬",
                "idle": "ğŸ’¤"
            }
            stage_text = {
                "greeting": "é—®å€™",
                "browsing": "æµè§ˆå•†å“",
                "selecting": "é€‰æ‹©å•†å“",
                "cart": "è´­ç‰©è½¦",
                "checkout": "ç»“è´¦",
                "tracking": "è®¢å•è·Ÿè¸ª",
                "service": "å®¢æœå’¨è¯¢",
                "idle": "ç©ºé—²"
            }
            ecommerce_context.append(f"{stage_emoji.get(stage, 'ğŸ“')} å½“å‰é˜¶æ®µ: {stage_text.get(stage, stage)}")
        
        # 2. æ˜¾ç¤ºæ„å›¾ä¿¡æ¯
        if intent_summary and intent_summary.get("current_intent"):
            current_intent = intent_summary["current_intent"]
            ecommerce_context.append(f"ğŸ¯ è¯†åˆ«æ„å›¾: {current_intent}")
            
            # æ˜¾ç¤ºé¢„æµ‹çš„ä¸‹ä¸€æ­¥
            predicted = intent_summary.get("predicted_next", [])
            if predicted:
                ecommerce_context.append(f"ğŸ”® å»ºè®®æ“ä½œ: {', '.join(predicted[:2])}")
        
        # 3. æ˜¾ç¤ºå¤åˆæ„å›¾ï¼ˆè´­ä¹°æ„å‘ç­‰ï¼‰
        if intent_summary and intent_summary.get("composite_intents"):
            for comp in intent_summary["composite_intents"]:
                comp_name = comp.get("name", "")
                if comp_name == "purchase_intent":
                    ecommerce_context.append("ğŸ’¡ æ£€æµ‹åˆ°è´­ä¹°æ„å‘")
                elif comp_name == "comparison_intent":
                    ecommerce_context.append("ğŸ”„ æ­£åœ¨æ¯”è¾ƒå•†å“")
                elif comp_name == "after_sales_intent":
                    ecommerce_context.append("ğŸ“ å…³æ³¨å”®åæœåŠ¡")
        
        # å°†ç”µå•†ä¸Šä¸‹æ–‡æ·»åŠ åˆ°å›å¤ä¸­ï¼ˆä½¿ç”¨å¼•ç”¨å—ï¼‰
        if ecommerce_context:
            context_text = "\n".join([f"> {line}" for line in ecommerce_context])
            final = f"{final}\n\n---\n**æ™ºèƒ½åŠ©æ‰‹çŠ¶æ€**\n{context_text}"
        
        # é€’å¢å¯¹è¯è®¡æ•°å™¨
        CONVERSATION_COUNTER += 1
        current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        # ç´¯ç§¯æ‰§è¡Œæ—¥å¿—åˆ°å†å² (æœ€æ–°çš„æ’å…¥åˆ°åˆ—è¡¨å¼€å¤´)
        current_execution_log = res.get("execution_log", [])
        if current_execution_log:
            EXECUTION_LOG_HISTORY.insert(0, {
                "conversation_num": CONVERSATION_COUNTER,
                "logs": current_execution_log,
                "start_time": current_time,
                "user_query": user_message
            })
        
        # ç´¯ç§¯ Plan å†å²
        PLAN_HISTORY.insert(0, {
            "conversation_num": CONVERSATION_COUNTER,
            "plan": plan,
            "start_time": current_time,
            "user_query": user_message
        })
        
        # ç´¯ç§¯ Tool Call å†å²
        TOOL_CALL_HISTORY.insert(0, {
            "conversation_num": CONVERSATION_COUNTER,
            "tool_calls": tool_calls,
            "start_time": current_time,
            "user_query": user_message
        })
        
        # æ ¼å¼åŒ–å†å²
        execution_log = format_execution_log_history()
        plan_display = format_plan_history()
        tool_display = format_tool_log_history()
        ecommerce_display = format_ecommerce_analysis()
        
        # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
        if res.get("error"):
            LOGGER.error(f"Agent æ‰§è¡Œå‡ºé”™: {res['error']}")
    except Exception as e:
        error_msg = f"å¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯: {type(e).__name__}: {str(e)}"
        LOGGER.error(error_msg, exc_info=True)
        final = f"âŒ {error_msg}"
        plan_display = f"## ğŸ“‹ Plan / Tasks\n\n**æ‰§è¡Œå¤±è´¥**: {error_msg}"
        tool_display = f"## ğŸ”§ Tool Calls\n\n**æ‰§è¡Œå¤±è´¥**: {error_msg}"
        execution_log = f"## ğŸ“‹ æ‰§è¡Œæ—¥å¿—å†å²\n\n**æ‰§è¡Œå¤±è´¥**: {error_msg}"
        ecommerce_display = f"## ğŸ›ï¸ ç”µå•†åˆ†æ\n\n**æ‰§è¡Œå¤±è´¥**: {error_msg}"

    assistant_placeholder["content"] = final
    memory_md = format_memory_context()
    
    return (
        gr.update(value=chat_history),
        gr.update(value=plan_display),
        gr.update(value=tool_display),
        gr.update(value=memory_md),
        gr.update(value=ecommerce_display),
        gr.update(value=execution_log),
    )


def clear_conversation():
    """æ¸…ç©ºå¯¹è¯å†å²å’Œè®°å¿†"""
    global EXECUTION_LOG_HISTORY, CONVERSATION_COUNTER, PLAN_HISTORY, TOOL_CALL_HISTORY
    
    AGENT.clear_memory()
    EXECUTION_LOG_HISTORY.clear()  # æ¸…ç©ºæ‰§è¡Œæ—¥å¿—å†å²
    PLAN_HISTORY.clear()  # æ¸…ç©ºè®¡åˆ’å†å²
    TOOL_CALL_HISTORY.clear()  # æ¸…ç©ºå·¥å…·è°ƒç”¨å†å²
    CONVERSATION_COUNTER = 0  # é‡ç½®å¯¹è¯è®¡æ•°å™¨
    LOGGER.info("å¯¹è¯å†å²ã€è®¡åˆ’ã€å·¥å…·è°ƒç”¨å’Œæ‰§è¡Œæ—¥å¿—å·²æ¸…ç©º")
    
    empty_memory = format_memory_context()
    empty_plan = "## ğŸ“‹ Plan / Tasks\n\n> *æš‚æ— è®¡åˆ’è®°å½•*"
    empty_tool = "## ğŸ”§ Tool Calls\n\n> *æš‚æ— å·¥å…·è°ƒç”¨è®°å½•*"
    empty_ecommerce = "## ğŸ›ï¸ ç”µå•†æ™ºèƒ½åˆ†æ\n\n> *å¼€å§‹å¯¹è¯åæ˜¾ç¤ºåˆ†ææ•°æ®*"
    empty_log = "## ğŸ“‹ æ‰§è¡Œæ—¥å¿—å†å²\n\n> *æš‚æ— æ‰§è¡Œè®°å½•*"
    return [], empty_plan, empty_tool, empty_memory, empty_ecommerce, empty_log


with gr.Blocks(
    title="Agent è¿è¡Œæ—¥å¿— UI",
    css="""
    /* å…è®¸é¡µé¢é«˜åº¦è¶…è¿‡é¦–å±å¹¶å¯ç”¨æ»šåŠ¨ */
    html, body {
        height: auto;
        min-height: 100vh;
        overflow-y: auto !important;
    }
    .gradio-container {
        min-height: 100vh;
        height: auto;
        display: flex;
        flex-direction: column;
    }
    .main-layout-row {
        gap: 16px;
    }
    .left-panel,
    .right-panel {
        display: flex;
        flex-direction: column;
        gap: 12px;
    }
    .tab-content {
        padding: 10px;
    }
    .quick-phrase-btn {
        font-size: 12px !important;
        padding: 4px 8px !important;
        min-width: 80px !important;
    }
    """
) as demo:
    gr.Markdown(f"# Ontology RL Commerce Agent \n**ChromaDB æŒä¹…åŒ–è®°å¿†** (ä¼šè¯: `{SESSION_ID[:12]}...`)")
    
    with gr.Row(equal_height=False, elem_classes="main-layout-row"):
        # å·¦ä¾§: èŠå¤©åŒºåŸŸ
        with gr.Column(scale=3, elem_classes="left-panel"):
            chatbot = gr.Chatbot(elem_id="mcp_chat", label="å¯¹è¯å†å²", height=600, type="messages")
            
            # ğŸ¯ ä¾¿æ·æµ‹è¯•çŸ­è¯­åŒºåŸŸ - 10ä¸ªå¿«æ·æŒ‰é’®
            gr.Markdown("### ğŸš€ å¿«æ·æµ‹è¯•çŸ­è¯­ï¼ˆç‚¹å‡»å³å¯æé—®ï¼‰")
            with gr.Row():
                quick_btn1 = gr.Button("ğŸ“Š æŸ¥è¯¢ç”¨æˆ·ç­‰çº§", elem_classes="quick-phrase-btn", size="sm")
                quick_btn2 = gr.Button("ğŸ æŸ¥è¯¢å¯ç”¨æŠ˜æ‰£", elem_classes="quick-phrase-btn", size="sm")
                quick_btn3 = gr.Button("ğŸšš æŸ¥è¯¢ç‰©æµæ–¹æ¡ˆ", elem_classes="quick-phrase-btn", size="sm")
                quick_btn4 = gr.Button("â†©ï¸ æŸ¥è¯¢é€€è´§æ”¿ç­–", elem_classes="quick-phrase-btn", size="sm")
                quick_btn5 = gr.Button("ğŸ“± æœç´¢iPhone", elem_classes="quick-phrase-btn", size="sm")
            with gr.Row():
                quick_btn6 = gr.Button("ğŸ›’ åˆ›å»ºæµ‹è¯•è®¢å•", elem_classes="quick-phrase-btn", size="sm")
                quick_btn7 = gr.Button("ğŸ” å•†å“è§„èŒƒåŒ–", elem_classes="quick-phrase-btn", size="sm")
                quick_btn8 = gr.Button("ğŸ›¡ï¸ SHACLæ ¡éªŒ", elem_classes="quick-phrase-btn", size="sm")
                quick_btn9 = gr.Button("ğŸ§  å®Œæ•´æ¨ç†æµç¨‹", elem_classes="quick-phrase-btn", size="sm")
                quick_btn10 = gr.Button("ğŸ“ˆ ç”¨æˆ·æ¶ˆè´¹åˆ†æ", elem_classes="quick-phrase-btn", size="sm")
            
            with gr.Row():
                txt = gr.Textbox(show_label=False, placeholder="åœ¨è¿™é‡Œè¾“å…¥ä½ çš„è¯·æ±‚", lines=2, scale=4)
                clear_btn = gr.Button("æ¸…ç©ºå¯¹è¯", variant="secondary", scale=1)
            submit = gr.Button("å‘é€", variant="primary")
            
        # å³ä¾§: Tab é¡µåˆ‡æ¢ (åŒ…å«æ‰€æœ‰è¾…åŠ©ä¿¡æ¯)
        with gr.Column(scale=2, elem_classes="right-panel"):
            with gr.Tabs(elem_classes="right-panel-scroll"):
                with gr.TabItem("ğŸ“‹ Plan / Tasks"):
                    plan_md = gr.Markdown("## ğŸ“‹ Plan / Tasks\n\n> *æš‚æ— è®¡åˆ’è®°å½•*", elem_id="plan_panel", elem_classes="tab-content")
                
                with gr.TabItem("ğŸ”§ Tool Calls"):
                    tool_md = gr.Markdown("## ğŸ”§ Tool Calls\n\n> *æš‚æ— å·¥å…·è°ƒç”¨è®°å½•*", elem_id="tool_panel", elem_classes="tab-content")
                
                with gr.TabItem("ğŸ’¾ Memory"):
                    memory_md = gr.Markdown(
                        value=format_memory_context(), 
                        elem_id="memory_panel",
                        elem_classes="tab-content"
                    )
                
                with gr.TabItem("ï¿½ï¸ ç”µå•†åˆ†æ"):
                    ecommerce_md = gr.Markdown(
                        "## ğŸ›ï¸ ç”µå•†æ™ºèƒ½åˆ†æ\n\n> *å¼€å§‹å¯¹è¯åæ˜¾ç¤ºåˆ†ææ•°æ®*",
                        elem_id="ecommerce_panel",
                        elem_classes="tab-content"
                    )
                
                with gr.TabItem("ï¿½ğŸ“Š Execution Log"):
                    execution_log_md = gr.Markdown(
                        "## ğŸ“‹ æ‰§è¡Œæ—¥å¿—å†å²\n\n> *æš‚æ— æ‰§è¡Œè®°å½•*", 
                        elem_id="execution_log_panel",
                        elem_classes="tab-content"
                    )

    def submit_and_update(message, history):
        """æäº¤æ¶ˆæ¯å¹¶æ›´æ–°æ‰€æœ‰é¢æ¿ - å…ˆæ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯ï¼Œå†è·å–å›å¤"""
        # ç¬¬ä¸€æ­¥ï¼šç«‹å³æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯ï¼ˆAssistantå›å¤ä¸º"æ€è€ƒä¸­..."ï¼‰å¹¶ç¦ç”¨æ‰€æœ‰æŒ‰é’®
        base_history = _normalize_chatbot_messages(history)
        pending_history = base_history + [
            {"role": "user", "content": message},
            {"role": "assistant", "content": "â³ æ­£åœ¨æ€è€ƒ..."},
        ]
        
        # ç«‹å³è¿”å›æ›´æ–°ï¼ˆç¦ç”¨æŒ‰é’®ï¼Œé˜²æ­¢é‡å¤æäº¤ï¼‰
        yield (
            gr.update(value=pending_history),  # chatbot
            gr.update(),  # plan_md (ä¿æŒä¸å˜)
            gr.update(),  # tool_md (ä¿æŒä¸å˜)
            gr.update(),  # memory_md (ä¿æŒä¸å˜)
            gr.update(),  # ecommerce_md (ä¿æŒä¸å˜)
            gr.update(),  # execution_log_md (ä¿æŒä¸å˜)
            gr.update(value="", interactive=False),  # æ¸…ç©ºè¾“å…¥æ¡†å¹¶ç¦ç”¨
            gr.update(interactive=False),  # ç¦ç”¨å‘é€æŒ‰é’®
            gr.update(interactive=False),  # ç¦ç”¨å¿«æ·æŒ‰é’®1
            gr.update(interactive=False),  # ç¦ç”¨å¿«æ·æŒ‰é’®2
            gr.update(interactive=False),  # ç¦ç”¨å¿«æ·æŒ‰é’®3
            gr.update(interactive=False),  # ç¦ç”¨å¿«æ·æŒ‰é’®4
            gr.update(interactive=False),  # ç¦ç”¨å¿«æ·æŒ‰é’®5
            gr.update(interactive=False),  # ç¦ç”¨å¿«æ·æŒ‰é’®6
            gr.update(interactive=False),  # ç¦ç”¨å¿«æ·æŒ‰é’®7
            gr.update(interactive=False),  # ç¦ç”¨å¿«æ·æŒ‰é’®8
            gr.update(interactive=False),  # ç¦ç”¨å¿«æ·æŒ‰é’®9
            gr.update(interactive=False),  # ç¦ç”¨å¿«æ·æŒ‰é’®10
        )
        
        # ç¬¬äºŒæ­¥ï¼šè°ƒç”¨åç«¯è·å–çœŸå®å›å¤
        result = handle_user_message(message, base_history)
        
        # ç¬¬ä¸‰æ­¥ï¼šè¿”å›å®Œæ•´ç»“æœï¼ˆå¯ç”¨æ‰€æœ‰æŒ‰é’®ï¼‰
        yield (
            result[0],  # chatbot (åŒ…å«çœŸå®å›å¤)
            result[1],  # plan_md
            result[2],  # tool_md
            result[3],  # memory_md
            result[4],  # ecommerce_md
            result[5],  # execution_log_md
            gr.update(value="", interactive=True),  # å¯ç”¨è¾“å…¥æ¡†
            gr.update(interactive=True),  # å¯ç”¨å‘é€æŒ‰é’®
            gr.update(interactive=True),  # å¯ç”¨å¿«æ·æŒ‰é’®1
            gr.update(interactive=True),  # å¯ç”¨å¿«æ·æŒ‰é’®2
            gr.update(interactive=True),  # å¯ç”¨å¿«æ·æŒ‰é’®3
            gr.update(interactive=True),  # å¯ç”¨å¿«æ·æŒ‰é’®4
            gr.update(interactive=True),  # å¯ç”¨å¿«æ·æŒ‰é’®5
            gr.update(interactive=True),  # å¯ç”¨å¿«æ·æŒ‰é’®6
            gr.update(interactive=True),  # å¯ç”¨å¿«æ·æŒ‰é’®7
            gr.update(interactive=True),  # å¯ç”¨å¿«æ·æŒ‰é’®8
            gr.update(interactive=True),  # å¯ç”¨å¿«æ·æŒ‰é’®9
            gr.update(interactive=True),  # å¯ç”¨å¿«æ·æŒ‰é’®10
        )
    
    # ğŸ¯ å¿«æ·çŸ­è¯­å‡½æ•° - é¢„è®¾æµ‹è¯•æŸ¥è¯¢ï¼ˆä½¿ç”¨ç”Ÿæˆå™¨ï¼‰
    def quick_phrase_1(history):
        """æŸ¥è¯¢ç”¨æˆ·ç­‰çº§æ¨ç†"""
        message = "æŸ¥è¯¢ç”¨æˆ·IDä¸º1çš„ç”¨æˆ·ç­‰çº§ï¼Œå¹¶è§£é‡Šæ¨ç†è¿‡ç¨‹"
        yield from submit_and_update(message, history)
    
    def quick_phrase_2(history):
        """æŸ¥è¯¢æŠ˜æ‰£æ¨ç†"""
        message = "ç”¨æˆ·ID 1è´­ä¹°é‡‘é¢15000å…ƒï¼ŒæŸ¥è¯¢å¯ç”¨çš„æŠ˜æ‰£ä¼˜æƒ ï¼Œå¹¶è§£é‡Šæ¨ç†ä¾æ®"
        yield from submit_and_update(message, history)
    
    def quick_phrase_3(history):
        """æŸ¥è¯¢ç‰©æµæ–¹æ¡ˆ"""
        message = "æŸ¥è¯¢ç”¨æˆ·ID 1çš„ç‰©æµé…é€æ–¹æ¡ˆï¼ŒåŒ…æ‹¬è¿è´¹å’Œé¢„è®¡é€è¾¾æ—¶é—´"
        yield from submit_and_update(message, history)
    
    def quick_phrase_4(history):
        """æŸ¥è¯¢é€€è´§æ”¿ç­–"""
        message = "ç”¨æˆ·ID 1è´­ä¹°äº†AirPods Pro 2ï¼ˆé…ä»¶ç±»å•†å“ï¼‰ï¼Œå·²æ‹†å°ä½†åŒ…è£…å®Œå¥½ï¼Œèƒ½å¦é€€è´§ï¼Ÿ"
        yield from submit_and_update(message, history)
    
    def quick_phrase_5(history):
        """æœç´¢å•†å“"""
        message = "æœç´¢iPhoneç›¸å…³çš„å•†å“ï¼Œæ˜¾ç¤ºåç§°ã€ä»·æ ¼å’Œåº“å­˜"
        yield from submit_and_update(message, history)
    
    def quick_phrase_6(history):
        """åˆ›å»ºæµ‹è¯•è®¢å•"""
        message = "ç”¨æˆ·ID 1è´­ä¹°2å°iPhone 15 Proï¼ˆå•†å“ID 2ï¼‰ï¼Œé…é€åœ°å€ï¼šæˆéƒ½æ­¦ä¾¯åŒºï¼Œç”µè¯ï¼š15308215756"
        yield from submit_and_update(message, history)
    
    def quick_phrase_7(history):
        """å•†å“è§„èŒƒåŒ–æµ‹è¯•"""
        message = "è§„èŒƒåŒ–æŸ¥è¯¢ï¼šè‹¹æœ15æ‰‹æœº"
        yield from submit_and_update(message, history)
    
    def quick_phrase_8(history):
        """SHACLæ ¡éªŒæµ‹è¯•"""
        message = "éªŒè¯è®¢å•æ•°æ®ï¼šç”¨æˆ·ID 1ï¼Œå•†å“ID 2ï¼Œæ•°é‡3ï¼Œåœ°å€æˆéƒ½ï¼Œç”µè¯15308215756ï¼Œæ˜¯å¦ç¬¦åˆSHACLè§„åˆ™"
        yield from submit_and_update(message, history)
    
    def quick_phrase_9(history):
        """å®Œæ•´æ¨ç†æµç¨‹"""
        message = "å®Œæ•´æ¼”ç¤ºæœ¬ä½“æ¨ç†æµç¨‹ï¼šç”¨æˆ·ID 1ï¼Œè®¢å•é‡‘é¢20000å…ƒï¼ŒåŒ…å«ç”¨æˆ·ç­‰çº§æ¨ç†ã€æŠ˜æ‰£è®¡ç®—ã€ç‰©æµæ–¹æ¡ˆã€SHACLæ ¡éªŒ"
        yield from submit_and_update(message, history)
    
    def quick_phrase_10(history):
        """ç”¨æˆ·æ¶ˆè´¹åˆ†æ"""
        message = "åˆ†æç”¨æˆ·ID 1çš„æ¶ˆè´¹æƒ…å†µï¼ŒåŒ…æ‹¬ç´¯è®¡æ¶ˆè´¹ã€ç­‰çº§å˜åŒ–å’Œæ¨èç­–ç•¥"
        yield from submit_and_update(message, history)

    # ç»‘å®šäº‹ä»¶ - è¾“å‡ºåŒ…å«æ‰€æœ‰éœ€è¦æ›´æ–°çš„ç»„ä»¶
    outputs = [
        chatbot, plan_md, tool_md, memory_md, ecommerce_md, execution_log_md, 
        txt, submit,  # è¾“å…¥æ¡†å’Œå‘é€æŒ‰é’®
        quick_btn1, quick_btn2, quick_btn3, quick_btn4, quick_btn5,  # å¿«æ·æŒ‰é’®
        quick_btn6, quick_btn7, quick_btn8, quick_btn9, quick_btn10
    ]
    
    submit.click(submit_and_update, [txt, chatbot], outputs)
    txt.submit(submit_and_update, [txt, chatbot], outputs)
    clear_btn.click(clear_conversation, None, [chatbot, plan_md, tool_md, memory_md, ecommerce_md, execution_log_md])
    
    # ğŸ¯ ç»‘å®šå¿«æ·æŒ‰é’®äº‹ä»¶
    quick_btn1.click(quick_phrase_1, [chatbot], outputs)
    quick_btn2.click(quick_phrase_2, [chatbot], outputs)
    quick_btn3.click(quick_phrase_3, [chatbot], outputs)
    quick_btn4.click(quick_phrase_4, [chatbot], outputs)
    quick_btn5.click(quick_phrase_5, [chatbot], outputs)
    quick_btn6.click(quick_phrase_6, [chatbot], outputs)
    quick_btn7.click(quick_phrase_7, [chatbot], outputs)
    quick_btn8.click(quick_phrase_8, [chatbot], outputs)
    quick_btn9.click(quick_phrase_9, [chatbot], outputs)
    quick_btn10.click(quick_phrase_10, [chatbot], outputs)


def _env_flag(name: str) -> bool:
    value = os.getenv(name)
    if value is None:
        return False
    return value.strip().lower() in {"1", "true", "yes", "on"}


if __name__ == "__main__":
    port = int(os.getenv("GRADIO_SERVER_PORT") or os.getenv("GRADIO_PORT", "7860"))
    server_name = os.getenv("GRADIO_SERVER_NAME", "127.0.0.1")
    share = _env_flag("GRADIO_SHARE")
    LOGGER.info("Launching Gradio UI on %s:%s (share=%s)", server_name, port, share)
    
    # å¯ç”¨é˜Ÿåˆ—ä»¥æ”¯æŒç”Ÿæˆå™¨å‡½æ•°ï¼ˆæ¸è¿›å¼UIæ›´æ–°ï¼‰
    demo.queue()
    
    launch_kwargs = {
        "server_name": server_name,
        "server_port": port,
        "share": share,
    }
    
    try:
        demo.launch(**launch_kwargs)
    except (ValueError, OSError) as exc:
        message = str(exc)
        if not share and "shareable link" in message.lower():
            LOGGER.warning("Localhost è®¿é—®å—é™ï¼Œè‡ªåŠ¨å›é€€ä¸º share=True")
            launch_kwargs["share"] = True
            demo.launch(**launch_kwargs)
        else:
            raise
